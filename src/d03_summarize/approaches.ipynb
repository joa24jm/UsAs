{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize the different split approaches for all datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ] Read in all datasets\n",
    "- [ ] Bring into one table\n",
    "- [ ] Format the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import helpers_summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the average rank of the approach with **time-sorted** users with its std?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average_rank</th>\n",
       "      <th>average_rank_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>time_cut</th>\n",
       "      <td>2.29</td>\n",
       "      <td>1.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bl_user_based_last</th>\n",
       "      <td>3.29</td>\n",
       "      <td>1.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bl_user_based_all</th>\n",
       "      <td>3.57</td>\n",
       "      <td>2.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_cut</th>\n",
       "      <td>3.57</td>\n",
       "      <td>1.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>average_user</th>\n",
       "      <td>3.86</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_wise</th>\n",
       "      <td>4.33</td>\n",
       "      <td>2.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bl_assessment_based_last</th>\n",
       "      <td>6.86</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bl_assessment_based_all</th>\n",
       "      <td>7.71</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          average_rank  average_rank_std\n",
       "time_cut                          2.29              1.50\n",
       "bl_user_based_last                3.29              1.70\n",
       "bl_user_based_all                 3.57              2.37\n",
       "user_cut                          3.57              1.72\n",
       "average_user                      3.86              0.69\n",
       "user_wise                         4.33              2.07\n",
       "bl_assessment_based_last          6.86              0.69\n",
       "bl_assessment_based_all           7.71              0.49"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "df = helpers_summarize.load_approach_tables(path='../../results/tables/approaches/sorted_users')\n",
    "\n",
    "# summarize and print\n",
    "results = helpers_summarize.prepare_results(df)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approaches where do not take users into account lead to an overestimation of the performance of the classifier in the testset. That is, if we allow users to be present in both the test and the train set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the average rank of the approach with **randomly-drawn** users with its std?\n",
    "\n",
    "Method: The whole ML pipeline for 9 datasets with 8 approaches each was repeated 5 times. Each time, a different seed was chosen to randomly draw train and test users.\n",
    "The overall question is: Do the approach rankings change if users are change from test to train sets and vice versa?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed_1962</th>\n",
       "      <th>seed_1964</th>\n",
       "      <th>seed_1991</th>\n",
       "      <th>seed_1994</th>\n",
       "      <th>seed_2023</th>\n",
       "      <th>mean_ranking</th>\n",
       "      <th>std_ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>time_cut</th>\n",
       "      <td>1.57</td>\n",
       "      <td>1.57</td>\n",
       "      <td>1.43</td>\n",
       "      <td>1.86</td>\n",
       "      <td>1.43</td>\n",
       "      <td>1.572</td>\n",
       "      <td>0.157022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_cut</th>\n",
       "      <td>3.14</td>\n",
       "      <td>3.14</td>\n",
       "      <td>3.14</td>\n",
       "      <td>2.86</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.056</td>\n",
       "      <td>0.112000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bl_user_based_last</th>\n",
       "      <td>3.29</td>\n",
       "      <td>3.43</td>\n",
       "      <td>3.43</td>\n",
       "      <td>3.29</td>\n",
       "      <td>3.86</td>\n",
       "      <td>3.460</td>\n",
       "      <td>0.209571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>average_user</th>\n",
       "      <td>3.43</td>\n",
       "      <td>3.71</td>\n",
       "      <td>3.86</td>\n",
       "      <td>3.71</td>\n",
       "      <td>2.86</td>\n",
       "      <td>3.514</td>\n",
       "      <td>0.355336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bl_user_based_all</th>\n",
       "      <td>4.29</td>\n",
       "      <td>4.29</td>\n",
       "      <td>4.57</td>\n",
       "      <td>4.29</td>\n",
       "      <td>4.71</td>\n",
       "      <td>4.430</td>\n",
       "      <td>0.177088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_wise</th>\n",
       "      <td>5.67</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4.50</td>\n",
       "      <td>5.17</td>\n",
       "      <td>5.17</td>\n",
       "      <td>5.102</td>\n",
       "      <td>0.375414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bl_assessment_based_last</th>\n",
       "      <td>6.86</td>\n",
       "      <td>6.57</td>\n",
       "      <td>6.86</td>\n",
       "      <td>6.86</td>\n",
       "      <td>6.86</td>\n",
       "      <td>6.802</td>\n",
       "      <td>0.116000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bl_assessment_based_all</th>\n",
       "      <td>7.43</td>\n",
       "      <td>7.86</td>\n",
       "      <td>7.71</td>\n",
       "      <td>7.57</td>\n",
       "      <td>7.71</td>\n",
       "      <td>7.656</td>\n",
       "      <td>0.145547</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          seed_1962  seed_1964  seed_1991  seed_1994  \\\n",
       "time_cut                       1.57       1.57       1.43       1.86   \n",
       "user_cut                       3.14       3.14       3.14       2.86   \n",
       "bl_user_based_last             3.29       3.43       3.43       3.29   \n",
       "average_user                   3.43       3.71       3.86       3.71   \n",
       "bl_user_based_all              4.29       4.29       4.57       4.29   \n",
       "user_wise                      5.67       5.00       4.50       5.17   \n",
       "bl_assessment_based_last       6.86       6.57       6.86       6.86   \n",
       "bl_assessment_based_all        7.43       7.86       7.71       7.57   \n",
       "\n",
       "                          seed_2023  mean_ranking  std_ranking  \n",
       "time_cut                       1.43         1.572     0.157022  \n",
       "user_cut                       3.00         3.056     0.112000  \n",
       "bl_user_based_last             3.86         3.460     0.209571  \n",
       "average_user                   2.86         3.514     0.355336  \n",
       "bl_user_based_all              4.71         4.430     0.177088  \n",
       "user_wise                      5.17         5.102     0.375414  \n",
       "bl_assessment_based_last       6.86         6.802     0.116000  \n",
       "bl_assessment_based_all        7.71         7.656     0.145547  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seeds = [1962, 1964, 1991, 1994, 2023]\n",
    "results_random = pd.DataFrame()\n",
    "for i, seed in enumerate(seeds):\n",
    "    # load data\n",
    "    df = helpers_summarize.load_approach_tables(path=f'../../results/tables/approaches/random_users/seed_{seed}')\n",
    "\n",
    "    # summarize and print\n",
    "    res = helpers_summarize.prepare_results(df)\n",
    "    \n",
    "    results_random[f'seed_{seed}'] = res['average_rank'] \n",
    "\n",
    "results_random['mean_ranking'] = results_random.mean(axis=1)\n",
    "results_random['std_ranking'] = results_random.std(axis=1)\n",
    "\n",
    "results_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
